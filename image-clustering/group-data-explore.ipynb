{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associative learning without a teacher - Techniques for grouping high dimensional objects #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will reduce our high dimensional feature vector representations of the image objects to two dimensions and explore geometrical and probabilistic ways of grouping or classifying the objects indepedently of a prior group information (i.e., class labels). This approach to learning the data structure is often referred to as unsupervised (as opposed to supervised and semi-supervised), which designates a leaning task that discovers patterns in the data independently of an explicit teacher.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize imports\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mahotas\n",
    "import h5py\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed-sizes for image\n",
    "fixed_size = tuple((250, 250))\n",
    "\n",
    "# path to training data\n",
    "datapath = \"dataset\"\n",
    "\n",
    "# bins for histogram\n",
    "bins = 8\n",
    "\n",
    "# seed for reproducing same results\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fd_hu_moments(image):\n",
    "    \"\"\"\n",
    "    Feature descriptor 1: Hu Moments for shape\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return feature\n",
    "\n",
    "def fd_haralick(image):\n",
    "    \"\"\" \n",
    "    Feature descriptor 2: Haralick Texture for surface texture\n",
    "    \"\"\"\n",
    "    # convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # compute the haralick texture feature vector\n",
    "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
    "    # return the result\n",
    "    return haralick\n",
    "\n",
    "def fd_histogram(image, mask=None):\n",
    "    \"\"\" \n",
    "    Feature descriptor 3: Color histogram for color\n",
    "    \"\"\"\n",
    "    # convert the image to HSV color-space\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # compute the color histogram\n",
    "    hist = cv2.calcHist(\n",
    "        [image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256]\n",
    "        )\n",
    "    # normalize the histogram\n",
    "    cv2.normalize(hist, hist)\n",
    "    # return the histogram\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = list()\n",
    "global_features = list()\n",
    "\n",
    "for fname in sorted(glob.glob(os.path.join(datapath, \"*.jpg\"))):\n",
    "    print(fname)\n",
    "    image = cv2.imread(fname)\n",
    "    image = cv2.resize(image, fixed_size)\n",
    "    fv_hu_moments = fd_hu_moments(image)\n",
    "    fv_haralick = fd_haralick(image)\n",
    "    fv_histogram = fd_histogram(image)\n",
    "    global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
    "    #  print(fv_histogram.shape)\n",
    "    #  print(fv_haralick.shape)\n",
    "    #  print(fv_hu_moments.shape)\n",
    "    global_features.append(global_feature)\n",
    "    image_names.append(fname.split(\"/\")[-1].split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaled_features = scaler.fit_transform(global_features)\n",
    "\n",
    "global_features = np.array(rescaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_names)\n",
    "print(global_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Each of our {} objects are represented as a 1x{} array\".format(global_features.shape[0],global_features.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "Dimensionality reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. Principal variables are a subset of the original variables and preserve, to some extent, the structure and information carried by the original variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2,random_state=0)\n",
    "pca.fit(global_features.T)\n",
    "#print(pca.explained_variance_ratio_)  \n",
    "#print(pca.singular_values_)\n",
    "\n",
    "print(pca.components_.shape)\n",
    "Y = pca.components_\n",
    "\n",
    "x_coords = Y[0,:]\n",
    "y_coords = Y[1,:]\n",
    "\n",
    "x_coords.shape\n",
    "\n",
    "artists = [name.split(\"_\")[0] for name in image_names]\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(artists)\n",
    "\n",
    "artist_label = list(zip(labels, artists))\n",
    "\n",
    "plt.scatter(x_coords, y_coords, c = labels)\n",
    "#plt.text(x_coords, y_coords, image_names)\n",
    "print(artist_label)\n",
    "\n",
    "for i, x in enumerate(x_coords):\n",
    "    y = y_coords[i]\n",
    "    l = artists[i]\n",
    "    plt.text(x,y,l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster-based grouping\n",
    "\n",
    "K-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(global_features)\n",
    "kmeans.labels_\n",
    "\n",
    "plt.scatter(x_coords, y_coords, c = kmeans.labels_)\n",
    "\n",
    "for i, x in enumerate(x_coords):\n",
    "    y = y_coords[i]\n",
    "    l = artists[i]\n",
    "    plt.text(x,y,l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca",
   "language": "python",
   "name": "ca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
